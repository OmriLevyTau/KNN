{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation  Notes\n",
    "\n",
    "Knn is straight forward algortihm. In order to classify a new data point, it finds the k nearest neighbors of that point\n",
    "and classifies according to the majority label.\n",
    "<br>\n",
    "Here are some notes regarding my Implementation:<br>\n",
    "##### note1:\n",
    "I'm using the trivial Euclidean distance. That is:\n",
    "$$ d(x,y) = \\sqrt{\\sum _{i}{(x_i-y_i)^2}} $$\n",
    "Which is the Euclidean Norm. <br>\n",
    "##### note2:\n",
    "Choosing the k-smallest elements in an array is an famous interesting issue:<br>\n",
    "1. Trivial: $O(k\\cdot n)$<br>\n",
    "Iterate k times and pick the next minimum element\n",
    "2. Better: $O(n\\cdot log(n))$<br>\n",
    "Sort the array keeping the original indices and pick the first k.\n",
    "3. Best: $O(n)$<br>\n",
    "This is the optimal solution Selection algorithm. Here I use numpy's partition which implements \"introselect\" algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNNClassifier:\n",
    "    def __init__(self, n_neighbors):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.data = np.empty((1,1))\n",
    "        self.labels = np.empty((1,1))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.data = X\n",
    "        self.labels= y\n",
    "        \n",
    "    def _predict_one_point(self,point):\n",
    "        dist = np.linalg.norm(self.data-point,axis=1) # note 1\n",
    "        k_smallets = np.argpartition(dist, self.n_neighbors)[:self.n_neighbors] #note 2\n",
    "        label_count = np.unique(self.labels[k_smallets],return_counts=True) \n",
    "        return label_count[0][label_count[1].argmax()]\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        preds = np.zeros((X.shape[0],1))\n",
    "        for i in range(X.shape[0]):\n",
    "            preds[i]=self._predict_one_point(X[i])\n",
    "        return preds.T[0]\n",
    "    \n",
    "    def score(self, predictions, true_labels):\n",
    "        \n",
    "        return (np.count_nonzero(predictions-true_labels.astype(\"int\"))/predictions.size)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are Testing its performence on the MNIST dataset while copmaring it to sklearn performence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', as_frame=False)\n",
    "data = mnist['data']\n",
    "labels = mnist['target']\n",
    "\n",
    "idx = np.random.RandomState(0).choice(70000, 11000)\n",
    "train = data[idx[:10000], :].astype(int)\n",
    "train_labels = labels[idx[:10000]]\n",
    "test = data[idx[10000:], :].astype(int)\n",
    "test_labels = labels[idx[10000:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = train[:1000],train_labels[:1000]\n",
    "accuracy_map = {\"k\":[], \"my_classifier\": [], \"sklearn_classifier\": []}\n",
    "for k in [1,2,5,10,30,60,100]:\n",
    "    accuracy_map[\"k\"].append(k)\n",
    "    \n",
    "    knn_b = kNNClassifier(k)\n",
    "    knn_b.fit(X_train,Y_train)\n",
    "    preds_b = knn_b.predict(test)\n",
    "    score_b = 1-knn_b.score(preds_b,test_labels)\n",
    "    accuracy_map[\"my_classifier\"].append(score_b)\n",
    "    \n",
    "    sklearn_knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    sklearn_knn.fit(X_train,Y_train)\n",
    "    sklearn_score = sklearn_knn.score(test,test_labels)\n",
    "    accuracy_map[\"sklearn_classifier\"].append(sklearn_score)\n",
    "\n",
    "accuracy_table = pd.DataFrame(accuracy_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='k', y=\"my_classifier\", data=accuracy_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(my_env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
